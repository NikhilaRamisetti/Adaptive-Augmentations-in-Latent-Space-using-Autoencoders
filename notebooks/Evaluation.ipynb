{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkG91uOAVT0V"
      },
      "source": [
        "# Evaluation of the Adaptive latent space augmentation encoder\n",
        "\n",
        "This notebook consists of the python scripts evaluating the trained model on face images.\n",
        "The evaluation notebook consists two out of  three subsections, each focusing on different aspects of the model's performance.\n",
        "\n",
        "5.1 Trained Model Performance\n",
        "  - MSE\n",
        "  - Perceptual Metrics - IS, FID\n",
        "  - Sample quality assessment (Reconstruction Examples)\n",
        "\n",
        "5.2 Latent space Exploration\n",
        "  - Visualizing Latent space using t-SNE or PCA\n",
        "  - Interpolation in Latent Space\n",
        "\n",
        "5.3 Computational Efficiency\n",
        "  - FLOPS calculations\n",
        "  - Model parameters estimation.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RNRlNjDXPsH"
      },
      "source": [
        "### Importing Libraries\n",
        "\n",
        "The below snippet imports necessary libraries including Tensorflow, scikit-learn, OpenCV, and Matplotlib for various tasks such as data preprocessing, model evaluation, and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pu9X_cuZXO0L"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "#import tensorflow_probability as tfp\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, Model, losses\n",
        "from tensorflow.keras.layers import Layer, Input, Conv2D, Dense, Flatten, Reshape, Lambda, Dropout\n",
        "from tensorflow.keras.layers import Conv2DTranspose, MaxPooling2D, UpSampling2D, LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.activations import relu\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOsrc9waX8De"
      },
      "source": [
        "### Custom layers and Model definition\n",
        "\n",
        "The below code snippet defines custom layers for the VAE model such as GaussianSampling, DownConvBlock, UpConvBlock, Encoder, Decoder, and the VAE model itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4RaRVmsX8TX"
      },
      "outputs": [],
      "source": [
        "class GaussianSampling(Layer):\n",
        "    def call(self, inputs):\n",
        "        means, logvar = inputs\n",
        "        epsilon = tf.random.normal(shape=tf.shape(means), mean=0., stddev=1.)\n",
        "        samples = means + tf.exp(0.5*logvar)*epsilon\n",
        "\n",
        "        return samples\n",
        "\n",
        "class DownConvBlock(Layer):\n",
        "    count = 0\n",
        "    def __init__(self, filters, kernel_size=(3,3), strides=1, padding='same'):\n",
        "        super(DownConvBlock, self).__init__(name=f\"DownConvBlock_{DownConvBlock.count}\")\n",
        "        DownConvBlock.count+=1\n",
        "        self.forward = Sequential([Conv2D(filters, kernel_size, strides, padding)])\n",
        "        self.forward.add(BatchNormalization())\n",
        "        self.forward.add(layers.LeakyReLU(0.2))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.forward(inputs)\n",
        "\n",
        "class UpConvBlock(Layer):\n",
        "    count = 0\n",
        "    def __init__(self, filters, kernel_size=(3,3), padding='same'):\n",
        "        super(UpConvBlock, self).__init__(name=f\"UpConvBlock_{UpConvBlock.count}\")\n",
        "        UpConvBlock.count += 1\n",
        "        self.forward = Sequential([Conv2D(filters, kernel_size, 1, padding),])\n",
        "        self.forward.add(layers.LeakyReLU(0.2))\n",
        "        self.forward.add(UpSampling2D((2,2)))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.forward(inputs)\n",
        "\n",
        "class Encoder(Layer):\n",
        "    def __init__(self, z_dim, name='encoder'):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "\n",
        "        self.features_extract = Sequential([\n",
        "            DownConvBlock(filters = 32, kernel_size=(3,3), strides=2),\n",
        "            DownConvBlock(filters = 32, kernel_size=(3,3), strides=2),\n",
        "            DownConvBlock(filters = 64, kernel_size=(3,3), strides=2),\n",
        "            DownConvBlock(filters = 64, kernel_size=(3,3), strides=2),\n",
        "            Flatten()])\n",
        "\n",
        "        self.dense_mean = Dense(z_dim, name='mean')\n",
        "        self.dense_logvar = Dense(z_dim, name='logvar')\n",
        "        self.sampler = GaussianSampling()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.features_extract(inputs)\n",
        "        mean = self.dense_mean(x)\n",
        "        logvar = self.dense_logvar(x)\n",
        "        z = self.sampler([mean, logvar])\n",
        "        return z, mean, logvar\n",
        "\n",
        "class Decoder(Layer):\n",
        "    def __init__(self, z_dim, name='decoder'):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "\n",
        "        self.forward = Sequential([\n",
        "                        Dense(8*8*64, activation='relu'),\n",
        "                        Reshape((8,8,64)),\n",
        "                        UpConvBlock(filters=64, kernel_size=(3,3)),\n",
        "                        UpConvBlock(filters=64, kernel_size=(3,3)),\n",
        "                        UpConvBlock(filters=32, kernel_size=(3,3)),\n",
        "                        UpConvBlock(filters=32, kernel_size=(3,3)),\n",
        "                        Conv2D(filters=3, kernel_size=(3,3), strides=1, padding='same', activation='sigmoid'),\n",
        "\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.forward(inputs)\n",
        "\n",
        "\n",
        "class VAE(Model):\n",
        "    def __init__(self, z_dim, name='VAE'):\n",
        "        super(VAE, self).__init__(name=name)\n",
        "        self.encoder = Encoder(z_dim)\n",
        "        self.decoder = Decoder(z_dim)\n",
        "        self.mean = None\n",
        "        self.logvar = None\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z, self.mean, self.logvar = self.encoder(inputs)\n",
        "        out = self.decoder(z)\n",
        "        return out\n",
        "\n",
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
        "    log2pi = tf.math.log(2. * np.pi)\n",
        "    return tf.reduce_sum(\n",
        "        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "        axis=raxis)\n",
        "\n",
        "def compute_loss(model, x):\n",
        "    z, mean, logvar = model.encoder(x)\n",
        "    x_logit = model.decoder(z)\n",
        "\n",
        "    # Reconstruction loss\n",
        "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
        "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
        "\n",
        "    # KL divergence loss\n",
        "    logpz = log_normal_pdf(z, 0., 0.)\n",
        "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
        "\n",
        "    # Total loss\n",
        "    loss = -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beMsyM5mZgeP"
      },
      "source": [
        "### Data and Model Loading\n",
        "\n",
        "This snippet loads the data, preprocesses by resizing and normalizing the images.\n",
        "It loads a pre-trained VAE model from a checkpoint file to evaluates its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "# Instantiate the model\n",
        "loaded_model = VAE(z_dim=64)  # Assuming z_dim is 128\n",
        "\n",
        "# Call the model once to build its variables\n",
        "sample_input = tf.ones((1, 128,128,3))  # Adjust input shape as per your model\n",
        "_ = loaded_model(sample_input)\n",
        "\n",
        "# Load weights from the checkpoint\n",
        "checkpoint_path = \"../data/ouput/checkpoints/final_model.h5\"\n",
        "loaded_model.load_weights(checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZwMm1PGZ9Lc"
      },
      "outputs": [],
      "source": [
        "num_files = 20000 # Define the desired number of files\n",
        "celebA_dir = '/content/drive/MyDrive/Master_project/CelebA_data/data/img_align_celeba'\n",
        "# List all JPEG files in the directory\n",
        "image_filenames = glob.glob(os.path.join(celebA_dir, '*.jpg'))\n",
        "\n",
        "# Take only the first 'num_files' files\n",
        "image_filenames = image_filenames[:num_files]\n",
        "\n",
        "# Check if there are any JPEG files found\n",
        "if not image_filenames:\n",
        "    raise ValueError(\"No JPEG files found in the directory.\")\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_files, test_files = train_test_split(image_filenames, test_size=0.9, random_state=42)\n",
        "\n",
        "# Print the number of images in each split\n",
        "print(\"Number of train images:\", len(train_files))\n",
        "print(\"Number of test images:\", len(test_files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_images(image_filenames, target_size=(128, 128)):\n",
        "    # Load, resize, and convert images to RGB\n",
        "    images = [cv2.cvtColor(cv2.resize(cv2.imread(filename), target_size), cv2.COLOR_BGR2RGB) for filename in image_filenames if cv2.imread(filename) is not None]\n",
        "    # Convert images to float32 and normalize\n",
        "    preprocessed_images = np.array(images).astype('float32') / 255.0\n",
        "    return preprocessed_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_size = (128,128)\n",
        "\n",
        "test_images = preprocess_images(test_files, target_size)\n",
        "print(f\"test_images:{test_images.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqUuCP4TaSsA"
      },
      "source": [
        "## 5.1 Trained Model performance\n",
        "\n",
        "We evaluate the trained model's effectiveness of generative models by employing various evaluation metrics. These metrics include mean squared error (MSE), perceptual metrics such as Incpetion Score (IS) and Frechet Inception distance (FID), as well as sample quality assessment.\n",
        "\n",
        "### Trained Model Performance metrics:\n",
        "\n",
        "- Mean Squared error:\n",
        "\n",
        "- Perceptuual metrics:\n",
        "  - Inception Score:\n",
        "  - Frechet Inception distance:\n",
        "\n",
        "- Sample quality evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_mse(images1, images2):\n",
        "    if images1.shape != images2.shape:\n",
        "        raise ValueError(\"Shapes of input images must be the same.\")\n",
        "    mse = np.mean((images1 - images2)**2)\n",
        "    rmse = np.sqrt(mse)\n",
        "    lmse = np.log(mse + 1e-9) # Adding a small epsilon to avoid log(0)\n",
        "    return mse, rmse, lmse\n",
        "\n",
        "# Function to compute validation loss\n",
        "def compute_validation_loss(model, validation_images):\n",
        "    z_test, encoded_imgs_mean, encoded_imgs_logvar = model.encoder(validation_images)\n",
        "    #z_test_augmented = latent_space_augmentations(z_test, cutout_mask_size, mixup_alpha)\n",
        "    # Generate fake images using the trained model\n",
        "    decoded_imgs = model.decoder(z_test)\n",
        "\n",
        "    # Convert images to NumPy arrays\n",
        "    test_images_np = validation_images\n",
        "    decoded_imgs_np = decoded_imgs.numpy()\n",
        "\n",
        "    # Compute mean squared error (MSE) as the validation loss\n",
        "    val_loss, rmse, lmse = compute_mse(test_images_np, decoded_imgs_np)\n",
        "    # Convert to TensorFlow tensor\n",
        "    val_loss_tf = tf.constant(val_loss, dtype=tf.float32)\n",
        "    rmse_tf = tf.constant(rmse, dtype = tf.float32)\n",
        "    lmse_tf = tf.constant(lmse, dtype = tf.float32)\n",
        "    return val_loss_tf, rmse_tf, lmse_tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MSE, RMSE, LMSE = compute_validation_loss(loaded_model, test_images)\n",
        "print(\"Trained Model Performance:\")\n",
        "print(f\"MSE:{MSE}, RMSE:{RMSE}, LMSE:{LMSE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_images(model, images):\n",
        "    z, mean, logvar = model.encoder(images)\n",
        "    images = model.decoder(z)\n",
        "    return images\n",
        "generated_images = generate_images(loaded_model, test_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inception Score, FID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Inception Score, FID\n",
        "from scipy.stats import entropy\n",
        "from skimage.transform import resize\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inception_score(images, inception_model, batch_size=16):\n",
        "    scores = []\n",
        "    for i in tqdm(range(0, len(images), batch_size)):\n",
        "        batch = images[i:i+batch_size]\n",
        "        batch = preprocess_input(batch)\n",
        "        preds = inception_model.predict(batch)\n",
        "        p_yx = preds.mean(axis=0)\n",
        "        kl_divs = []\n",
        "        for pred in preds:\n",
        "            kl_divs.append(entropy(pred, p_yx))\n",
        "        scores.append(np.exp(np.mean(kl_divs)))  # Append to the list instead of extending\n",
        "    return np.mean(scores), np.std(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.linalg import sqrtm\n",
        "\n",
        "def calculate_fid(real_images, generated_images, inception_model):\n",
        "    # Resize images to 299x299 as required by InceptionV3\n",
        "    real_resized = np.array([resize(image, (128, 128, 3)) for image in real_images])\n",
        "    gen_resized = np.array([resize(image, (128, 128, 3)) for image in generated_images])\n",
        "\n",
        "    # Preprocess images\n",
        "    real_preprocessed = preprocess_input(real_resized)\n",
        "    gen_preprocessed = preprocess_input(gen_resized)\n",
        "\n",
        "    # Get feature representations\n",
        "    real_features = inception_model.predict(real_preprocessed)\n",
        "    gen_features = inception_model.predict(gen_preprocessed)\n",
        "\n",
        "    # Calculate mean and covariance\n",
        "    mu_real, sigma_real = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
        "    mu_gen, sigma_gen = gen_features.mean(axis=0), np.cov(gen_features, rowvar=False)\n",
        "\n",
        "    # Calculate FID\n",
        "    diff = mu_real - mu_gen\n",
        "    cov_sqrt = sqrtm(sigma_real.dot(sigma_gen))  # Use scipy's square root function\n",
        "    if np.iscomplexobj(cov_sqrt):\n",
        "        cov_sqrt = cov_sqrt.real\n",
        "    fid = np.dot(diff, diff) + np.trace(sigma_real + sigma_gen - 2 * cov_sqrt)\n",
        "    return fid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "\n",
        "# Load the InceptionV3 model\n",
        "inception_model = InceptionV3(include_top=False, pooling='avg', input_shape=(128, 128, 3))\n",
        "\n",
        "# Calculate and print Inception Score\n",
        "inception_score_mean, inception_score_std = inception_score(generated_images, inception_model)\n",
        "print(\"Inception Score:\", inception_score_mean, \"+/-\", inception_score_std)\n",
        "\n",
        "# Calculate and print FID\n",
        "fid = calculate_fid(test_images, generated_images, inception_model)\n",
        "print(\"FID:\", fid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample quality generation\n",
        "\n",
        "This code snippet visualizes pairs of original images and their reconstructions generated by a Variational Autoencoder (VAE) model. It defines a function to plot the original and reconstructed images side by side, and then selects a subset of test images to visualize. Finally, it calls the visualization function with the VAE model and the selected subset of images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to visualize input images and their reconstructions\n",
        "def visualize_reconstructions(model, images, num_images=10):\n",
        "    # Generate reconstructions\n",
        "    reconstructed_images = model(images)\n",
        "\n",
        "    # Plot original images and their reconstructions\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    for i in range(num_images):\n",
        "        # Original image\n",
        "        plt.subplot(2, num_images, i + 1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.title('Original')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Reconstructed image\n",
        "        plt.subplot(2, num_images, num_images + i + 1)\n",
        "        plt.imshow(reconstructed_images[i])\n",
        "        plt.title('Reconstructed')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Choose the number of images to visualize (e.g., 10 or 16)\n",
        "num_images_to_visualize = 10\n",
        "\n",
        "# Select a subset of test images for visualization\n",
        "subset_images_for_visualization = test_images[:num_images_to_visualize]\n",
        "\n",
        "# Visualize input images and their reconstructions\n",
        "visualize_reconstructions(loaded_model, subset_images_for_visualization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Latent Space Analysis\n",
        "\n",
        "### T-SNE of latent space of VAE trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Load data and encode it using the encoder\n",
        "# Assuming you have a dataset named 'x_train'\n",
        "encoded_data, _, _ = loaded_model.encoder(test_images)\n",
        "\n",
        "# Apply t-SNE to reduce the dimensionality of the latent space representations to two dimensions\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "latent_space_tsne = tsne.fit_transform(encoded_data)\n",
        "\n",
        "# Visualize the t-SNE embeddings using a scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(latent_space_tsne[:, 0], latent_space_tsne[:, 1], c='b', alpha=0.5)\n",
        "plt.title('t-SNE Visualization of Latent Space')\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exploring latent space for generated faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# example of loading the generator model and generating images\n",
        "from numpy import asarray\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.models import load_model\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        " # generate points in the latent space\n",
        " x_input = randn(latent_dim * n_samples)\n",
        " # reshape into a batch of inputs for the network\n",
        " z_input = x_input.reshape(n_samples, latent_dim)\n",
        " return z_input\n",
        "\n",
        "# create a plot of generated images\n",
        "def plot_generated(examples, n):\n",
        " # plot images\n",
        " for i in range(n * n):\n",
        "  # define subplot\n",
        "  pyplot.subplot(n, n, 1 + i)\n",
        "  # turn off axis\n",
        "  pyplot.axis('off')\n",
        "  # plot raw pixel data\n",
        "  pyplot.imshow(examples[i, :, :])\n",
        "pyplot.show()\n",
        "\n",
        "# generate images\n",
        "latent_points = generate_latent_points(64, 25)\n",
        "# generate images\n",
        "X  = loaded_model.decoder(latent_points)\n",
        "# scale from [-1,1] to [0,1]\n",
        "X = (X + 1) / 2.0\n",
        "# plot the result\n",
        "plot_generated(X, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from numpy import linspace\n",
        "\n",
        "# uniform interpolation between two points in latent space\n",
        "def interpolate_points(p1, p2, n_steps=9):\n",
        "  # interpolate ratios between the points\n",
        "  ratios = linspace(0, 1, num=n_steps)\n",
        "  # linear interpolate vectors\n",
        "  vectors = list()\n",
        "  for ratio in ratios:\n",
        "    v = (1.0 - ratio) * p1 + ratio * p2\n",
        "    vectors.append(v)\n",
        "  return asarray(vectors)\n",
        "\n",
        "pts = generate_latent_points(64, 2)\n",
        "# interpolate points in latent space\n",
        "interpolated = interpolate_points(pts[0], pts[1])\n",
        "# generate images\n",
        "try:\n",
        "    X = loaded_model.decoder(interpolated)\n",
        "    # Scale from [-1,1] to [0,1]\n",
        "    X = (X + 1) / 2.0\n",
        "    # Print shapes\n",
        "    print(\"Shape of interpolated images:\", X.shape)\n",
        "    # Plot the result\n",
        "    plot_generated(X, len(interpolated))\n",
        "except Exception as e:\n",
        "    print(\"Error occurred during interpolation:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computational analysis \n",
        "\n",
        "Many deep learning research papers specifically report the following metrics to compare Time complexity(Speed of inference) and space complexity (Model size).\n",
        "\n",
        "1. Time complexity in terms of FLOPS (floating-point operations) \n",
        "2. Model size in terms of the number of parameters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FLOPS Calculation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def get_flops(model, model_inputs) -> float:\n",
        "        \"\"\"\n",
        "        Calculate FLOPS [GFLOPs] for a tf.keras.Model or tf.keras.Sequential model\n",
        "        in inference mode. It uses tf.compat.v1.profiler under the hood.\n",
        "        \"\"\"\n",
        "        # if not hasattr(model, \"model\"):\n",
        "        #     raise wandb.Error(\"self.model must be set before using this method.\")\n",
        "\n",
        "        if not isinstance(\n",
        "            model, (tf.keras.models.Sequential, tf.keras.models.Model)\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                \"Calculating FLOPS is only supported for \"\n",
        "                \"`tf.keras.Model` and `tf.keras.Sequential` instances.\"\n",
        "            )\n",
        "\n",
        "        from tensorflow.python.framework.convert_to_constants import (\n",
        "            convert_variables_to_constants_v2_as_graph,\n",
        "        )\n",
        "\n",
        "        # Compute FLOPs for one sample\n",
        "        batch_size = 1\n",
        "        inputs = [\n",
        "            tf.TensorSpec([batch_size] + inp.shape[1:], inp.dtype)\n",
        "            for inp in model_inputs\n",
        "        ]\n",
        "\n",
        "        # convert tf.keras model into frozen graph to count FLOPs about operations used at inference\n",
        "        real_model = tf.function(model).get_concrete_function(inputs)\n",
        "        frozen_func, _ = convert_variables_to_constants_v2_as_graph(real_model)\n",
        "\n",
        "        # Calculate FLOPs with tf.profiler\n",
        "        run_meta = tf.compat.v1.RunMetadata()\n",
        "        opts = (\n",
        "            tf.compat.v1.profiler.ProfileOptionBuilder(\n",
        "                tf.compat.v1.profiler.ProfileOptionBuilder().float_operation()\n",
        "            )\n",
        "            .with_empty_output()\n",
        "            .build()\n",
        "        )\n",
        "\n",
        "        flops = tf.compat.v1.profiler.profile(\n",
        "            graph=frozen_func.graph, run_meta=run_meta, cmd=\"scope\", options=opts\n",
        "        )\n",
        "\n",
        "        tf.compat.v1.reset_default_graph()\n",
        "\n",
        "        # convert to GFLOPs\n",
        "        return flops.total_float_ops, (flops.total_float_ops / 1e9)/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Usage\n",
        "if __name__ ==\"__main__\":\n",
        "    image_model = tf.keras.applications.EfficientNetB0(include_top=False, weights=None)\n",
        "\n",
        "    x = tf.constant(np.random.randn(1,128,128,3))\n",
        "\n",
        "    print(get_flops(loaded_model, [x]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model size in terms of parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Count total number of parameters\n",
        "total_params = loaded_model.count_params()\n",
        "\n",
        "# Count trainable parameters\n",
        "trainable_params = sum([tf.keras.backend.count_params(w) for w in loaded_model.trainable_weights])\n",
        "\n",
        "print(\"Total number of parameters:\", total_params)\n",
        "print(\"Total number of trainable parameters:\", trainable_params)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
